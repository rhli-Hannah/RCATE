n <- nrow(x)
p <- ncol(x)
# if(is.null(nsis)) nsis=min(floor(n/log(n)),p-1)
if (is.null(knots)) {
knots = ceiling(n^0.2)
}
if (is.null(folds)) {
temp = sample(1:n, n, replace = FALSE)
if (is.null(kfold))
kfold = 5
for (i in 1:kfold) {
folds[[i]] = setdiff(1:n, temp[seq(i, n, kfold)])
}
}
if (is.null(quant)) {
quant = 1
}
df0 <- knots + 1
xbs = matrix(0, n, df0 * p)
for (i in 1:p) {
xbs[, (i - 1) * (df0) + (1:df0)] = splines::ns(x[, i], df = df0)
}
tempresi <- rep(0, p)
curloop = 1
for (i in 1:p) {
tempfit <- stats::lm.fit(x = cbind(1, xbs[, (i - 1) * df0 + 1:df0]), y = y)
tempresi[i] <- sum(tempfit$residuals^2)
}
used.list <- tempresi
used.sort <- sort(used.list, method = "sh", index = TRUE, decreasing = FALSE)
initRANKorder <- used.sort$ix
mindex <- sample(1:n)
mresi = NULL
for (i in 1:p) {
tempfit <- stats::lm.fit(x = cbind(1, xbs[, (i - 1) * df0 + 1:df0]), y = y[mindex])
mresi[i] <- sum(tempfit$residuals^2)
}
resi.thres = stats::quantile(mresi, 1 - quant)
nsis <- max(min(sum(used.list < resi.thres), floor(n/df0/3)), 2)
SISind <- sort(initRANKorder[1:nsis])
if (!DOISIS)
return(list(initRANKorder = initRANKorder, SISind = SISind, nsis = nsis))
cat("loop ", curloop, "...SISind ", SISind, "\n")
pick.ind = initRANKorder[1:nsis]
return(pick.ind)
}
#' d = rbinom(n,1,p)
#' t = 2*d-1
#' y = 100+4*X[,1]+tau*t/2 + rnorm(n,0,1)
#' x_val = matrix(rnorm(200*3,0,1),nrow=200,ncol=3)
#' tau_val = 6*sin(2*x_val[,1])+3*(x_val[,2])
#'
#' fit <- rcate.am(X,y,d)
#' y_pred <- predict(fit,x_val)$pred
#' plot(tau_val,y_pred);abline(0,1)
#' @export
rcate.am <- function(x, y, d, method = "MCMEA", NIS = TRUE, nknots = NA,
lambda.smooth = 1, nlambda = 30, nfolds = 5, n.trees.p = 40000,
shrinkage.p = 0.005, n.minobsinnode.p = 10,
interaction.depth.p = 1, cv.p = 2, n.trees.mu = c(1:50) * 50,
shrinkage.mu = 0.01,
n.minobsinnode.mu = 5, interaction.depth.mu = 5, cv.mu = 5) {
# Calculate T=2D-1
t <- 2 * d - 1
if (is.vector(x)) {
x <- matrix(x, ncol = 1)
}
# Number of rows and columns of X
row <- nrow(x)
col <- ncol(x)
# Standardize X
mean.x <- apply(x, 2, mean)
sd.x <- apply(x, 2, sd)
center.x <- (x - mean.x)/sd.x
# Calculate number of knots
if (is.na(nknots)) {
nknots <- floor(sqrt(row)/2)
}
# NIS
if (col < floor(row/log(row)) | NIS == FALSE) {
colnum <- 1:col
} else {
colnum_d <- adaptINIS(x, d)
colnum_y1 <- adaptINIS(x[d == 1, ], y[d == 1])
colnum_y0 <- adaptINIS(x[d == 0, ], y[d == 0])
colnum <- sort(unique(union(union(colnum_d, colnum_y0), colnum_y1)))
}
group <- rep(1:length(colnum), each = nknots + 3)
x.tr <- center.x[, colnum]
# If X has only one dimension, transform it into a matrix with one column
if (is.vector(x.tr)) {
x.tr <- matrix(x.tr, ncol = 1)
}
# Estimate mu0(x), mu1(x) and p(x)
data.p <- data.frame(cbind(factor(d), x))
colnames(data.p) <- c("d", paste0("X", 1:ncol(x)))
data.p$d <- as.factor(data.p$d)
gbmGrid.p <- expand.grid(interaction.depth = interaction.depth.p,
n.trees = n.trees.p, shrinkage = shrinkage.p,
n.minobsinnode = n.minobsinnode.p)
gbmFit.p <- caret::train(d ~ ., data = data.p, method = "gbm",
verbose = FALSE, trControl = caret::trainControl(method = "cv", number = cv.p),
tuneGrid = gbmGrid.p)
pscore.hat <- caret::predict.train(gbmFit.p, newdata = data.p, type = "prob")[, 2]
data00 <- data.frame(cbind(y, x, d))
colnames(data00) <- c("y", paste0("X", 1:ncol(x)), "d")
data020 <- data00[data00$d == 0, ]
data021 <- data00[data00$d == 1, ]
gbmGrid.mu <- expand.grid(interaction.depth = interaction.depth.mu,
n.trees = n.trees.mu, shrinkage = shrinkage.mu,
n.minobsinnode = n.minobsinnode.mu)
gbmFit.mu <- caret::train(y ~ ., data = data00[, -ncol(data00)],
method = "gbm", verbose = FALSE, trControl = caret::trainControl(method = "cv",
number = cv.mu), tuneGrid = gbmGrid.mu, metric = "MAE")
gbmFit.mu1 <- caret::train(y ~ ., data = data021[, -ncol(data021)],
method = "gbm", verbose = FALSE, trControl = caret::trainControl(method = "cv",
number = cv.mu), tuneGrid = gbmGrid.mu, metric = "MAE")
gbmFit.mu0 <- caret::train(y ~ ., data = data020[, -ncol(data020)],
method = "gbm", verbose = FALSE, trControl = caret::trainControl(method = "cv",
number = cv.mu), tuneGrid = gbmGrid.mu, metric = "MAE")
mu0 <- caret::predict.train(gbmFit.mu0, newdata = data00)
mu1 <- caret::predict.train(gbmFit.mu1, newdata = data00)
mu.ea <- caret::predict.train(gbmFit.mu, newdata = data00)
# Do transformation
if (method == "MCMEA") {
w.tr <- 1/(t * pscore.hat + (1 - t)/2)
y.tr <- (y - mu.ea) * w.tr
wmat.tr <- matrix(0, row, row)
diag(wmat.tr) <- w.tr * t/2
Btilde <- B_R(x.tr, x.tr, lambda.smooth, nknots, colnum)
x.tr1 <- wmat.tr %*% cbind(rep(1, row), Btilde)
} else if (method == "RL") {
w.tr <- 1
y.tr <- (y_tr - mu.ea) * w.tr
wmat.tr <- matrix(0, row, row)
diag(wmat.tr) <- w.tr * (t - 2 * pscore.hat + 1)/2
Btilde <- B_R(x.tr, x.tr, lambda2, nknots, colnum)
x.tr1 <- wmat.tr %*% cbind(rep(1, row), Btilde)
} else if (method == "DR") {
y.tr <- (t - 2 * pscore.hat + 1) * (y)/(2 * pscore.hat * (1 - pscore.hat)) +
(pscore.hat - d)/pscore.hat * mu1 + (pscore.hat - d)/(1 - pscore.hat) * mu0
w.tr <- abs((t - 2 * pscore.hat + 1)/(2 * pscore.hat * (1 - pscore.hat)))
}
# Fit the weighted LAD model with group SCAD
model <- rqPen::cv.rq.group.pen(x = x.tr1, y = y.tr, groups = as.factor(c(max(group)+1, group)),
tau = 0.5, intercept = FALSE, penalty = 'SCAD',
nfolds = nfolds, criteria = "BIC", nlambda = nlambda)
# Get coefficients and fitted value
coef <- coef(model)
fitted.values <- cbind(rep(1, nrow(Btilde)), Btilde) %*% coef
result <- list(model = model, method = method, algorithm = "SAM",
lambda.smooth = lambda.smooth, fitted.values = fitted.values,
x = x, y = y, d = d, mean.x = mean.x, sd.x = sd.x,
coef = coef, colnum = colnum, nknots = nknots)
class(result) <- "rcate.am"
return(result)
}
# Use L1 MCM-EA and additive model to estimate CATE
n <- 500; p <- 3; set.seed(2222)
X <- matrix(rnorm(n*p,0,1),nrow=n,ncol=p)
tau = 6*sin(2*X[,1])+3*(X[,2])
p = 1/(1+exp(-X[,1]+X[,2]))
d = rbinom(n,1,p)
t = 2*d-1
y = 100+4*X[,1]+tau*t/2 + rnorm(n,0,1); set.seed(2223)
x_val = matrix(rnorm(200*p,0,1),nrow=200,ncol=p)
tau_val = 6*sin(2*x_val[,1])+3*(x_val[,2])
x_val = matrix(rnorm(200*p,0,1),nrow=200,ncol=p)
# Use L1 MCM-EA and additive model to estimate CATE
n <- 500; p <- 3; set.seed(2222)
X <- matrix(rnorm(n*p,0,1),nrow=n,ncol=p)
tau = 6*sin(2*X[,1])+3*(X[,2])
p = 1/(1+exp(-X[,1]+X[,2]))
d = rbinom(n,1,p)
t = 2*d-1
y = 100+4*X[,1]+tau*t/2 + rnorm(n,0,1); set.seed(2223)
x_val = matrix(rnorm(200*3,0,1),nrow=200,ncol=3)
tau_val = 6*sin(2*x_val[,1])+3*(x_val[,2])
fit <- rcate.am(X,y,d)
y_pred <- predict(fit,x_val)$pred
plot(tau_val,y_pred);abline(0,1)
```
<img src="man/figures/README-example-1.png" width="60%" />
# Use L1 MCM-EA and additive model to estimate CATE
n <- 500; p <- 3; set.seed(2223)
X <- matrix(rnorm(n*p,0,1),nrow=n,ncol=p)
tau = 6*sin(2*X[,1])+3*(X[,2])
p = 1/(1+exp(-X[,1]+X[,2]))
d = rbinom(n,1,p)
t = 2*d-1
y = 100+4*X[,1]+tau*t/2 + rnorm(n,0,1); set.seed(2223)
x_val = matrix(rnorm(200*3,0,1),nrow=200,ncol=3)
tau_val = 6*sin(2*x_val[,1])+3*(x_val[,2])
fit <- rcate.am(X,y,d)
y_pred <- predict(fit,x_val)$pred
plot(tau_val,y_pred);abline(0,1)
```
# Use L1 MCM-EA and additive model to estimate CATE
n <- 1000; p <- 3; set.seed(2220)
X <- matrix(rnorm(n*p,0,1),nrow=n,ncol=p)
tau = 6*sin(2*X[,1])+3*(X[,2])
p = 1/(1+exp(-X[,1]+X[,2]))
d = rbinom(n,1,p)
t = 2*d-1
y = 100+4*X[,1]+tau*t/2 + rnorm(n,0,1); set.seed(2223)
x_val = matrix(rnorm(200*3,0,1),nrow=200,ncol=3)
tau_val = 6*sin(2*x_val[,1])+3*(x_val[,2])
fit <- rcate.am(X,y,d)
y_pred <- predict(fit,x_val)$pred
plot(tau_val,y_pred);abline(0,1)
```
## basic example
n <- 1000; p <- 10; set.seed(2220)
X <- matrix(rnorm(n*p,0,1),nrow=n,ncol=p)
tau = 6*sin(2*X[,1])+3*(X[,2]+3)*X[,3]+9*tanh(0.5*X[,4])+3*X[,5]*(2*I(X[,4]<1)-1)
p = 1/(1+exp(-X[,1]+X[,2]))
d = rbinom(n,1,p)
t = 2*d-1
y = 100+4*X[,1]+X[,2]-3*X[,3]+tau*t/2 + rnorm(n,0,1); set.seed(2223)
x_val = matrix(rnorm(200*10,0,1),nrow=200,ncol=10)
tau_val = 6*sin(2*x_val[,1])+3*(x_val[,2]+3)*x_val[,3]+9*tanh(0.5*x_val[,4])+
3*x_val[,5]*(2*I(x_val[,4]<1)-1)
# Use L1 R-learning and GBM to estimate CATE
fit <- rcate.ml(X,y,d,method='RL',algorithm='GBM')
y_pred <- predict(fit,x_val)$predict
plot(tau_val,y_pred);abline(0,1)
# Use L1 doubly robust method and neural network to estimate CATE
fit <- rcate.ml(X,y,d,method='DR',algorithm='NN',dropout.nn=c(0,0))
y_pred <- predict(fit,x_val)$predict
plot(tau_val,y_pred);abline(0,1)
# Use L1 doubly robust method and random forests to estimate CATE
fit <- rcate.rf(X,y,d,newdata=data.frame(x_val),method='DR')
y_pred <- fit$pred
plot(tau_val,y_pred);abline(0,1)
## basic example
n <- 1000; p <- 10; set.seed(2222)
X <- matrix(rnorm(n*p,0,1),nrow=n,ncol=p)
tau = 6*sin(2*X[,1])+3*(X[,2]+3)*X[,3]+9*tanh(0.5*X[,4])+3*X[,5]*(2*I(X[,4]<1)-1)
p = 1/(1+exp(-X[,1]+X[,2]))
d = rbinom(n,1,p)
t = 2*d-1
y = 100+4*X[,1]+X[,2]-3*X[,3]+tau*t/2 + rnorm(n,0,1); set.seed(2223)
x_val = matrix(rnorm(200*10,0,1),nrow=200,ncol=10)
tau_val = 6*sin(2*x_val[,1])+3*(x_val[,2]+3)*x_val[,3]+9*tanh(0.5*x_val[,4])+
3*x_val[,5]*(2*I(x_val[,4]<1)-1)
# Use L1 R-learning and GBM to estimate CATE
fit <- rcate.ml(X,y,d,method='RL',algorithm='GBM')
y_pred <- predict(fit,x_val)$predict
plot(tau_val,y_pred);abline(0,1)
# Use L1 doubly robust method and random forests to estimate CATE
fit <- rcate.rf(X,y,d,newdata=data.frame(x_val),method='DR')
y_pred <- fit$pred
plot(tau_val,y_pred);abline(0,1)
## basic example
n <- 1000; p <- 5; set.seed(2222)
X <- matrix(rnorm(n*p,0,1),nrow=n,ncol=p)
tau = 6*sin(2*X[,1])+3*(X[,2]+3)*X[,3]+9*tanh(0.5*X[,4])+3*X[,5]*(2*I(X[,4]<1)-1)
p = 1/(1+exp(-X[,1]+X[,2]))
d = rbinom(n,1,p)
t = 2*d-1
y = 100+4*X[,1]+X[,2]-3*X[,3]+tau*t/2 + rnorm(n,0,1); set.seed(2223)
x_val = matrix(rnorm(200*5,0,1),nrow=200,ncol=5)
tau_val = 6*sin(2*x_val[,1])+3*(x_val[,2]+3)*x_val[,3]+9*tanh(0.5*x_val[,4])+
3*x_val[,5]*(2*I(x_val[,4]<1)-1)
# Use L1 R-learning and GBM to estimate CATE
fit <- rcate.ml(X,y,d,method='RL',algorithm='GBM')
y_pred <- predict(fit,x_val)$predict
plot(tau_val,y_pred);abline(0,1)
# Use L1 doubly robust method and neural network to estimate CATE
fit <- rcate.ml(X,y,d,method='DR',algorithm='NN',dropout.nn=c(0,0))
y_pred <- predict(fit,x_val)$predict
plot(tau_val,y_pred);abline(0,1)
# Use L1 doubly robust method and random forests to estimate CATE
fit <- rcate.rf(X,y,d,newdata=data.frame(x_val),method='DR')
y_pred <- fit$pred
plot(tau_val,y_pred);abline(0,1)
n <- 1000; p <- 10
X <- matrix(rnorm(n*p,0,1),nrow=n,ncol=p)
tau = 6*sin(2*X[,1])+3*(X[,2]+3)*X[,3]+9*tanh(0.5*X[,4])+3*X[,5]*(2*I(X[,4]<1)-1)
p = 1/(1+exp(-X[,1]+X[,2]))
d = rbinom(n,1,p)
t = 2*d-1
y = 100+4*X[,1]+X[,2]-3*X[,3]+tau*t/2 + rnorm(n,0,1)
x_val = matrix(rnorm(200*10,0,1),nrow=200,ncol=10)
tau_val = 6*sin(2*x_val[,1])+3*(x_val[,2]+3)*x_val[,3]+9*tanh(0.5*x_val[,4])+
3*x_val[,5]*(2*I(x_val[,4]<1)-1)
# Use MCM-EA transformation and GBM to estimate CATE
fit <- rcate.rf(X,y,d,newdata=data.frame(x_val),method='DR')
y_pred <- fit$pred
plot(tau_val,y_pred);abline(0,1)
#' y = 100+4*X[,1]+X[,2]-3*X[,3]+tau*t/2 + rnorm(n,0,1)
#' x_val = matrix(rnorm(200*10,0,1),nrow=200,ncol=10)
#' tau_val = 6*sin(2*x_val[,1])+3*(x_val[,2]+3)*x_val[,3]+9*tanh(0.5*x_val[,4])+
#' 3*x_val[,5]*(2*I(x_val[,4]<1)-1)
#'
#' # Use MCM-EA transformation and GBM to estimate CATE
#' fit <- rcate.rf(X,y,d,newdata=data.frame(x_val),method='DR')
#' y_pred <- fit$pred
#' plot(tau_val,y_pred);abline(0,1)
#' @export
rcate.rf <- function(x, y, d, method = "MCMEA",
n.trees.p = 40000, shrinkage.p = 0.005, n.minobsinnode.p = 10,
interaction.depth.p = 1, cv.p = 5, n.trees.mu = c(1:50) * 50,
shrinkage.mu = 0.01, n.minobsinnode.mu = 5,
interaction.depth.mu = 5, cv.mu = 5,
n.trees.rf = 50, feature.frac = 1/2, newdata = NULL, minnodes = 5) {
# Calculate T=2D-1
t <- 2 * d - 1
# Estimate mu0(x), mu1(x) and p(x)
data.p <- data.frame(cbind(d, x))
#colnames(data.p) <- c("d", paste0("X", 1:ncol(x)))
data.p$d <- as.factor(data.p$d)
gbmGrid.p <- expand.grid(interaction.depth = interaction.depth.p,
n.trees = n.trees.p, shrinkage = shrinkage.p,
n.minobsinnode = n.minobsinnode.p)
gbmFit.p <- caret::train(d ~ ., data = data.p, method = "gbm",
verbose = FALSE,
trControl = caret::trainControl(method = "cv", number = cv.p),
tuneGrid = gbmGrid.p)
pscore.hat <- caret::predict.train(gbmFit.p, newdata = data.p, type = "prob")[, 2]
data00 <- data.frame(cbind(y, x, d))
colnames(data00) <- c("y", paste0("X", 1:ncol(x)), "d")
data020 <- data00[data00$d == 0, ]
data021 <- data00[data00$d == 1, ]
gbmGrid.mu <- expand.grid(interaction.depth = interaction.depth.mu,
n.trees = n.trees.mu, shrinkage = shrinkage.mu,
n.minobsinnode = n.minobsinnode.mu)
gbmFit.mu <- caret::train(y ~ ., data = data00[, -ncol(data00)],
method = "gbm", verbose = FALSE,
trControl = caret::trainControl(method = "cv",
number = cv.mu), tuneGrid = gbmGrid.mu, metric = "MAE")
gbmFit.mu1 <- caret::train(y ~ ., data = data021[, -ncol(data021)],
method = "gbm", verbose = FALSE,
trControl = caret::trainControl(method = "cv",
number = cv.mu), tuneGrid = gbmGrid.mu, metric = "MAE")
gbmFit.mu0 <- caret::train(y ~ ., data = data020[, -ncol(data020)],
method = "gbm", verbose = FALSE,
trControl = caret::trainControl(method = "cv",
number = cv.mu), tuneGrid = gbmGrid.mu, metric = "MAE")
mu0 <- caret::predict.train(gbmFit.mu0, newdata = data00)
mu1 <- caret::predict.train(gbmFit.mu1, newdata = data00)
mu.ea <- caret::predict.train(gbmFit.mu, newdata = data00)
# Do transformation
if (method == "MCMEA") {
y.tr <- 2 * t * (y - mu.ea)
w.tr <- 1/2 * (t * pscore.hat + (1 - t)/2)
} else if (method == "RL") {
y.tr <- 2 * (y - mu.ea)/(t - 2 * pscore.hat + 1)
w.tr <- abs(t - 2 * pscore.hat + 1)/2
} else if (method == "DR") {
y.tr <- (t - 2 * pscore.hat + 1) * (y)/(2 * pscore.hat * (1 - pscore.hat)) +
(pscore.hat - d)/pscore.hat * mu1 + (pscore.hat - d)/(1 - pscore.hat) * mu0
w.tr <- abs((t - 2 * pscore.hat + 1)/(2 * pscore.hat * (1 - pscore.hat)))
}
data.rf <-  data.frame(y.tr,x)
if (is.null(newdata)) {newdata.rf <- data.rf} else {newdata.rf <- data.frame(newdata)}
formula.rf <- stats::as.formula(paste0('y.tr', " ~ ",paste0(colnames(data.frame(x)), collapse = " + ")))
result <- reg_rf(formula=formula.rf, n_trees = n.trees.rf, feature_frac = feature.frac,
data=data.rf, newdata=newdata.rf, weights = w.tr, minnodes = 5)
return(result)
}
fit <- rcate.rf(X,y,d,newdata=data.frame(x_val),method='DR',n.trees.rf=5)
y_pred <- fit$pred
plot(tau_val,y_pred);abline(0,1)
#' y = 100+4*X[,1]+X[,2]-3*X[,3]+tau*t/2 + rnorm(n,0,1)
#' x_val = matrix(rnorm(200*10,0,1),nrow=200,ncol=10)
#' tau_val = 6*sin(2*x_val[,1])+3*(x_val[,2]+3)*x_val[,3]+9*tanh(0.5*x_val[,4])+
#' 3*x_val[,5]*(2*I(x_val[,4]<1)-1)
#'
#' # Use MCM-EA transformation and GBM to estimate CATE
#' fit <- rcate.rf(X,y,d,newdata=data.frame(x_val),method='DR')
#' y_pred <- fit$pred
#' plot(tau_val,y_pred);abline(0,1)
#' @export
rcate.rf <- function(x, y, d, method = "MCMEA",
n.trees.p = 40000, shrinkage.p = 0.005, n.minobsinnode.p = 10,
interaction.depth.p = 1, cv.p = 5, n.trees.mu = c(1:50) * 50,
shrinkage.mu = 0.01, n.minobsinnode.mu = 5,
interaction.depth.mu = 5, cv.mu = 5,
n.trees.rf = 50, feature.frac = 0.8, newdata = NULL, minnodes = 3) {
# Calculate T=2D-1
t <- 2 * d - 1
# Estimate mu0(x), mu1(x) and p(x)
data.p <- data.frame(cbind(d, x))
#colnames(data.p) <- c("d", paste0("X", 1:ncol(x)))
data.p$d <- as.factor(data.p$d)
gbmGrid.p <- expand.grid(interaction.depth = interaction.depth.p,
n.trees = n.trees.p, shrinkage = shrinkage.p,
n.minobsinnode = n.minobsinnode.p)
gbmFit.p <- caret::train(d ~ ., data = data.p, method = "gbm",
verbose = FALSE,
trControl = caret::trainControl(method = "cv", number = cv.p),
tuneGrid = gbmGrid.p)
pscore.hat <- caret::predict.train(gbmFit.p, newdata = data.p, type = "prob")[, 2]
data00 <- data.frame(cbind(y, x, d))
colnames(data00) <- c("y", paste0("X", 1:ncol(x)), "d")
data020 <- data00[data00$d == 0, ]
data021 <- data00[data00$d == 1, ]
gbmGrid.mu <- expand.grid(interaction.depth = interaction.depth.mu,
n.trees = n.trees.mu, shrinkage = shrinkage.mu,
n.minobsinnode = n.minobsinnode.mu)
gbmFit.mu <- caret::train(y ~ ., data = data00[, -ncol(data00)],
method = "gbm", verbose = FALSE,
trControl = caret::trainControl(method = "cv",
number = cv.mu), tuneGrid = gbmGrid.mu, metric = "MAE")
gbmFit.mu1 <- caret::train(y ~ ., data = data021[, -ncol(data021)],
method = "gbm", verbose = FALSE,
trControl = caret::trainControl(method = "cv",
number = cv.mu), tuneGrid = gbmGrid.mu, metric = "MAE")
gbmFit.mu0 <- caret::train(y ~ ., data = data020[, -ncol(data020)],
method = "gbm", verbose = FALSE,
trControl = caret::trainControl(method = "cv",
number = cv.mu), tuneGrid = gbmGrid.mu, metric = "MAE")
mu0 <- caret::predict.train(gbmFit.mu0, newdata = data00)
mu1 <- caret::predict.train(gbmFit.mu1, newdata = data00)
mu.ea <- caret::predict.train(gbmFit.mu, newdata = data00)
# Do transformation
if (method == "MCMEA") {
y.tr <- 2 * t * (y - mu.ea)
w.tr <- 1/2 * (t * pscore.hat + (1 - t)/2)
} else if (method == "RL") {
y.tr <- 2 * (y - mu.ea)/(t - 2 * pscore.hat + 1)
w.tr <- abs(t - 2 * pscore.hat + 1)/2
} else if (method == "DR") {
y.tr <- (t - 2 * pscore.hat + 1) * (y)/(2 * pscore.hat * (1 - pscore.hat)) +
(pscore.hat - d)/pscore.hat * mu1 + (pscore.hat - d)/(1 - pscore.hat) * mu0
w.tr <- abs((t - 2 * pscore.hat + 1)/(2 * pscore.hat * (1 - pscore.hat)))
}
data.rf <-  data.frame(y.tr,x)
if (is.null(newdata)) {newdata.rf <- data.rf} else {newdata.rf <- data.frame(newdata)}
formula.rf <- stats::as.formula(paste0('y.tr', " ~ ",paste0(colnames(data.frame(x)), collapse = " + ")))
result <- reg_rf(formula=formula.rf, n_trees = n.trees.rf, feature_frac = feature.frac,
data=data.rf, newdata=newdata.rf, weights = w.tr, minnodes = 5)
return(result)
}
fit <- rcate.rf(X,y,d,newdata=data.frame(x_val),method='DR')
y_pred <- fit$pred
plot(tau_val,y_pred);abline(0,1)
n <- 1000; p <- 5; set.seed(2222)
X <- matrix(rnorm(n*p,0,1),nrow=n,ncol=p)
tau = 6*sin(2*X[,1])+3*(X[,2]+3)*X[,3]+9*tanh(0.5*X[,4])+3*X[,5]*(2*I(X[,4]<1)-1)
p = 1/(1+exp(-X[,1]+X[,2]))
d = rbinom(n,1,p)
t = 2*d-1
y = 100+4*X[,1]+X[,2]-3*X[,3]+tau*t/2 + rnorm(n,0,1); set.seed(2223)
x_val = matrix(rnorm(200*5,0,1),nrow=200,ncol=5)
tau_val = 6*sin(2*x_val[,1])+3*(x_val[,2]+3)*x_val[,3]+9*tanh(0.5*x_val[,4])+
3*x_val[,5]*(2*I(x_val[,4]<1)-1)
# Use L1 doubly robust method and random forests to estimate CATE
fit <- rcate.rf(X,y,d,newdata=data.frame(x_val),method='DR')
y_pred <- fit$pred
plot(tau_val,y_pred);abline(0,1)
n <- 1000; p <- 5; set.seed(2222)
X <- matrix(rnorm(n*p,0,1),nrow=n,ncol=p)
tau = 6*sin(2*X[,1])+3*(X[,2]+3)*X[,3]+9*tanh(0.5*X[,4])+3*X[,5]*(2*I(X[,4]<1)-1)
p = 1/(1+exp(-X[,1]+X[,2]))
d = rbinom(n,1,p)
t = 2*d-1
y = 100+4*X[,1]+X[,2]-3*X[,3]+tau*t/2 + rnorm(n,0,1); set.seed(2223)
x_val = matrix(rnorm(200*5,0,1),nrow=200,ncol=5)
tau_val = 6*sin(2*x_val[,1])+3*(x_val[,2]+3)*x_val[,3]+9*tanh(0.5*x_val[,4])+
3*x_val[,5]*(2*I(x_val[,4]<1)-1)
# Use L1 doubly robust method and random forests to estimate CATE
fit <- rcate.rf(X,y,d,newdata=data.frame(x_val),method='DR',feature.frac = 0.8, minnodes = 5)
y_pred <- fit$pred
plot(tau_val,y_pred);abline(0,1)
# Use L1 doubly robust method and random forests to estimate CATE
fit <- rcate.rf(X,y,d,newdata=data.frame(x_val),method='DR',feature.frac = 0.8, minnodes = 3)
y_pred <- fit$pred
plot(tau_val,y_pred);abline(0,1)
n <- 1000; p <- 10; set.seed(2222)
X <- matrix(rnorm(n*p,0,1),nrow=n,ncol=p)
tau = 6*sin(2*X[,1])+3*(X[,2]+3)*X[,3]+9*tanh(0.5*X[,4])+3*X[,5]*(2*I(X[,4]<1)-1)
p = 1/(1+exp(-X[,1]+X[,2]))
d = rbinom(n,1,p)
t = 2*d-1
y = 100+4*X[,1]+X[,2]-3*X[,3]+tau*t/2 + rnorm(n,0,1); set.seed(2223)
x_val = matrix(rnorm(200*10,0,1),nrow=200,ncol=10)
tau_val = 6*sin(2*x_val[,1])+3*(x_val[,2]+3)*x_val[,3]+9*tanh(0.5*x_val[,4])+
3*x_val[,5]*(2*I(x_val[,4]<1)-1)
# Use L1 doubly robust method and random forests to estimate CATE
fit <- rcate.rf(X,y,d,newdata=data.frame(x_val),method='DR',feature.frac = 0.8, minnodes = 3)
y_pred <- fit$pred
plot(tau_val,y_pred);abline(0,1)
n <- 1000; p <- 3; set.seed(2222)
X <- matrix(rnorm(n*p,0,1),nrow=n,ncol=p)
tau = 6*sin(2*X[,1])+3*(X[,2])
p = 1/(1+exp(-X[,1]+X[,2]))
d = rbinom(n,1,p)
t = 2*d-1
y = 100+4*X[,1]+tau*t/2 + rnorm(n,0,1); set.seed(2223)
x_val = matrix(rnorm(200*3,0,1),nrow=200,ncol=3)
tau_val = 6*sin(2*x_val[,1])+3*(x_val[,2])
fit <- rcate.am(X,y,d)
y_pred <- predict(fit,x_val)$pred
plot(tau_val,y_pred);abline(0,1)
n <- 1000; p <- 5
X <- matrix(rnorm(n*p,0,1),nrow=n,ncol=p); set.seed(2222)
tau = 6*sin(2*X[,1])+3*(X[,2]+3)*X[,3]+9*tanh(0.5*X[,4])+3*X[,5]*(2*I(X[,4]<1)-1)
p = 1/(1+exp(-X[,1]+X[,2]))
d = rbinom(n,1,p)
t = 2*d-1
y = 100+4*X[,1]+X[,2]-3*X[,3]+tau*t/2 + rnorm(n,0,1); set.seed(2223)
x_val = matrix(rnorm(200*5,0,1),nrow=200,ncol=5)
tau_val = 6*sin(2*x_val[,1])+3*(x_val[,2]+3)*x_val[,3]+9*tanh(0.5*x_val[,4])+
3*x_val[,5]*(2*I(x_val[,4]<1)-1)
# Use MCM-EA transformation and GBM to estimate CATE
fit <- rcate.rf(X,y,d,newdata=data.frame(x_val),method='DR',feature.frac = 0.8, minnodes = 5)
y_pred <- fit$pred
plot(tau_val,y_pred);abline(0,1)
devtools::install_github("rhli-Hannah/RCATE")
library(RCATE)
