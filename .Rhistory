data021 <- data00[data00$d == 1, ]
gbmGrid.mu <- expand.grid(interaction.depth = interaction.depth.mu,
n.trees = n.trees.mu, shrinkage = shrinkage.mu,
n.minobsinnode = n.minobsinnode.mu)
gbmFit.mu <- caret::train(y ~ ., data = data00[, -ncol(data00)],
method = "gbm", verbose = FALSE,
trControl = trainControl(method = "cv",
number = cv.mu), tuneGrid = gbmGrid.mu, metric = "MAE")
gbmFit.mu1 <- caret::train(y ~ ., data = data021[, -ncol(data021)],
method = "gbm", verbose = FALSE,
trControl = trainControl(method = "cv",
number = cv.mu), tuneGrid = gbmGrid.mu, metric = "MAE")
gbmFit.mu0 <- caret::train(y ~ ., data = data020[, -ncol(data020)],
method = "gbm", verbose = FALSE,
trControl = trainControl(method = "cv",
number = cv.mu), tuneGrid = gbmGrid.mu, metric = "MAE")
mu0 <- predict(gbmFit.mu0, newdata = data00)
mu1 <- predict(gbmFit.mu1, newdata = data00)
mu.ea <- predict(gbmFit.mu, newdata = data00)
# Do transformation
if (method == "MCMEA") {
y.tr <- 2 * t * (y - mu.ea)
w.tr <- 1/2 * (t * pscore.hat + (1 - t)/2)
} else if (method == "RL") {
y.tr <- 2 * (y - mu.ea)/(t - 2 * pscore.hat + 1)
w.tr <- abs(t - 2 * pscore.hat + 1)/2
} else if (method == "DR") {
y.tr <- (t - 2 * pscore.hat + 1) * (y)/(2 * pscore.hat * (1 - pscore.hat)) +
(pscore.hat - d)/pscore.hat * mu1 + (pscore.hat - d)/(1 - pscore.hat) * mu0
w.tr <- abs((t - 2 * pscore.hat + 1)/(2 * pscore.hat * (1 - pscore.hat)))
}
data.rf <-  data.frame(y.tr,x)
if (is.null(newdata)) {newdata.rf <- data.rf} else {newdata.rf <- data.frame(newdata)}
formula.rf <- as.formula(paste0('y.tr', " ~ ",paste0(colnames(data.frame(x)), collapse = " + ")))
result <- reg_rf(formula=formula.rf, n_trees = n.trees.rf, feature_frac = feature.frac,
data=data.rf, newdata=newdata.rf, weights = w.tr, minnodes = 5)
return(result)
}
n <- 1000; p <- 10
X <- matrix(rnorm(n*p,0,1),nrow=n,ncol=p)
tau = 6*sin(2*X[,1])+3*(X[,2]+3)*X[,3]+9*tanh(0.5*X[,4])+3*X[,5]*(2*I(X[,4]<1)-1)
p = 1/(1+exp(-X[,1]+X[,2]))
d = rbinom(n,1,p)
t = 2*d-1
y = 100+4*X[,1]+X[,2]-3*X[,3]+tau*t/2 + rnorm(n,0,1)
x_val = matrix(rnorm(200*10,0,1),nrow=200,ncol=10)
tau_val = 6*sin(2*x_val[,1])+3*(x_val[,2]+3)*x_val[,3]+9*tanh(0.5*x_val[,4])+3*x_val[,5]*(2*I(x_val[,4]<1)-1)
fit <- rcate.rf(X,y,d)
y_pred <- fit$pred
plot(tau,y_pred);abline(0,1)
fit <- rcate.rf(X,y,d,newdata=data.frame(x_val))
y_pred <- fit$pred
plot(tau_val,y_pred);abline(0,1)
library(MASS)
#library(CVTuningCov)
library(gbm)
#library(keras)
#library(tensorflow)
library(caret)
library(plyr)
library(dplyr)
library(randomForest)
library(rmutil)
# general learner with ML
# Implement DR, RL, MCM-EA with random forests, XGB, and NN
# In simulation, use mixture of Normal and
# (1) t(2) (2) Normal with large variance (3) Laplace distribution
# mu, mu0, mu1 and p are estimated by XGB with MAE and RMSE as metrics correspondingly
# input X, T (D), Y
source('/Users/rhli/OneDrive/research/2020SPRING/myfun.nonlinear.R')
source('/Users/rhli/OneDrive/research/2020SPRING/functions/rt_rf.R')
# source('/gpfs/home/r/h/rhli/Carbonate/ML/myfun.nonlinear.R')
# source('/gpfs/home/r/h/rhli/Carbonate/ML/rt_rf.R')
row=1000;col=10;outlier=TRUE;outlier.prop=0;sd0=1;simu=50;val_row=1000
set.seed(2223)
x_val = generatex(row0=val_row,col0=col)
tau_val = tau(x=x_val)#;hist(tau_val)
x_tr = generatex(row0=row,col0=col)
tau_tr = tau(x=x_tr)
p = propensity(x=x_tr)
d_tr = rbinom(row,1,p)
t_tr = 2*d_tr-1
u = runif(row,0,1)
error = rnorm(row,0,sd=sd0)
lengthu = length(u[u>(1-outlier.prop)])
if (outlier==TRUE) {
error[u>(1-outlier.prop)] <-
# rnorm(lengthu,0,10)
# rt(lengthu,2)
rlaplace(lengthu,0,sqrt(50))
}
y_tr = baseline(x_tr)+tau_tr*t_tr/2 + error
# Estimate mu0(x), mu1(x) and p(x)
data01 <- data.frame(cbind(factor(d_tr),x_tr))
data01$X1 <- as.factor(data01$X1)
gbmGrid2 <- expand.grid(interaction.depth=c(1),
n.trees=40000,#40000,
shrinkage=c(0.0005),
n.minobsinnode=c(10))
gbmFit2 <- caret::train(X1~.,data=data01,method='gbm',verbose=FALSE,
trControl=trainControl(method='cv',number=5),
tuneGrid=gbmGrid2)
pscore.hat <- predict(gbmFit2,newdata = data01,type = 'prob')[,2]
#plot(p,pscore.hat);abline(0,1)
#pscore.hat <- p
#mse.p <- mean((pscore.hat-p)^2)
data00 <- data.frame(cbind(y_tr,x_tr,d_tr))
data02 <- data00
data020 <- data02[data02$d_tr==0,]
data021 <- data02[data02$d_tr==1,]
gbmGrid1 <- expand.grid(interaction.depth=c(5),
n.trees=c(1:50)*50,
shrinkage=c(0.01),
n.minobsinnode=5)
gbmFit1 <- caret::train(y_tr~.,data=data020[,-ncol(data020)],method='gbm',verbose=FALSE,
trControl=trainControl(method='cv',number=5),
tuneGrid=gbmGrid1,metric='MAE')
gbmFit3 <- caret::train(y_tr~.,data=data021[,-ncol(data021)],method='gbm',verbose=FALSE,
trControl=trainControl(method='cv',number=5),
tuneGrid=gbmGrid1,metric='MAE')
gbmFit4 <- caret::train(y_tr~.,data=data02[,-ncol(data02)],method='gbm',verbose=FALSE,
trControl=trainControl(method='cv',number=5),
tuneGrid=gbmGrid1,metric='MAE')
mu0 <- predict(gbmFit1,newdata = data00)
#mu0 <- baseline(x_tr)-1/2*tau(x_tr)
#mu0.true <- baseline(x_tr)-1/2*tau(x_tr)
#plot(mu0.true,mu0);abline(0,1)
mu1 <- predict(gbmFit3,newdata = data00)
#mu1 <- baseline(x_tr)+1/2*tau(x_tr)
#mu1.true <- baseline(x_tr)+1/2*tau(x_tr)
#plot(mu1.true,mu1);abline(0,1)
mu.ea <- predict(gbmFit4,newdata = data00)
#mu.ea <- baseline(x_tr)+tau_tr*(p-0.5)
#mu <- baseline(x_tr)+tau_tr*(p-0.5)
#plot(mu,mu.ea);abline(0,1)
# Outcome transformation for DR estimator
y.dr <- (t_tr-2*pscore.hat+1)*(y_tr)/(2*pscore.hat*(1-pscore.hat))+
(pscore.hat-d_tr)/pscore.hat*mu1+(pscore.hat-d_tr)/(1-pscore.hat)*mu0
w.dr.l2 <- rep(1,row)
w.dr.l1 <- abs((t_tr-2*pscore.hat+1)/(2*pscore.hat*(1-pscore.hat)))
# Outcome transformation for IPW estimator
# y.ipw <- (t_tr-2*pscore.hat+1)*(y_tr)/(2*pscore.hat*(1-pscore.hat))
# w.ipw.l2 <- rep(1,row)
# w.ipw.l1 <- abs((t_tr-2*pscore.hat+1)/(2*pscore.hat*(1-pscore.hat)))
# Outcome transformation for MCM estimator
# y.mcm <- 2*t_tr*(y_tr)
# w.mcm.l2 <- 1/(t_tr*pscore.hat+(1-t_tr)/2)
# w.mcm.l1 <- 1/(t_tr*pscore.hat+(1-t_tr)/2)
# Outcome transformation for MCM-EA estimator
y.mcmea <- 2*t_tr*(y_tr-mu.ea)
w.mcmea.l2 <- 1/4*(t_tr*pscore.hat+(1-t_tr)/2)
w.mcmea.l1 <- 1/2*(t_tr*pscore.hat+(1-t_tr)/2)
# Outcome transformation for AL estimator
# y.al <- 2*(y_tr)/(t_tr-2*pscore.hat+1)
# w.al.l2 <- 1/(t_tr-2*pscore.hat+1)^2
# w.al.l1 <- 1/abs(t_tr-2*pscore.hat+1)
# Outcome transformation for RL estimator
y.rl <- 2*(y_tr-mu.ea)/(t_tr-2*pscore.hat+1)
w.rl.l2 <- (t_tr-2*pscore.hat+1)^2/4
w.rl.l1 <- abs(t_tr-2*pscore.hat+1)/2
# weighted quantile forest
# rf.dt <- model.matrix(~(X1+X2+X3+X4+X5)^2,data.frame(x_tr))
# rf.dt.val <- model.matrix(~(X1+X2+X3+X4+X5)^2,data.frame(x_val))
mod_rf.mcmea <- reg_rf(formula = y.mcmea~ V2+V3+V4+V5+V6+V7+V8+V9+V10+V11,
# X1+X2+X3+X4+X5+X1.X2+X1.X3+X1.X4+X1.X5+
# X2.X3+X2.X4+X2.X5+X3.X4+X3.X5+X4.X5,
data = data.frame(cbind(y.mcmea,x_tr)), n_trees = 50,
feature_frac = 0.8, newdata = data.frame(cbind(tau_val,x_val)),
weights = w.mcmea.l1,minnodes = 5)
pred5.mcmea <- mod_rf.mcmea$pred
plot(tau_val,pred5.mcmea);abline(0,1)
# weighted quantile forest
mod_rf.rl <- reg_rf(formula = y.rl~V2+V3+V4+V5+V6+V7+V8+V9+V10+V11,
data = data.frame(cbind(y.rl,x_tr)), n_trees = 50,
feature_frac = 0.8, newdata = data.frame(cbind(tau_val,x_val)),
weights = w.rl.l1,minnodes = 5)
pred5.rl <- mod_rf.rl$pred
plot(tau_val,pred5.rl);abline(0,1)
fit <- rcate.rf(X,y,d,newdata=data.frame(x_val),method='RL')
y_pred <- fit$pred
plot(tau_val,y_pred);abline(0,1)
n <- 1000; p <- 10
X <- matrix(rnorm(n*p,0,1),nrow=n,ncol=p)
tau = 6*sin(2*X[,1])+3*(X[,2]+3)*X[,3]+9*tanh(0.5*X[,4])+3*X[,5]*(2*I(X[,4]<1)-1)
p = 1/(1+exp(-X[,1]+X[,2]))
d = rbinom(n,1,p)
t = 2*d-1
y = 100+4*X[,1]+X[,2]-3*X[,3]+tau*t/2 + rnorm(n,0,1)
x_val = matrix(rnorm(200*10,0,1),nrow=200,ncol=10)
tau_val = 6*sin(2*x_val[,1])+3*(x_val[,2]+3)*x_val[,3]+9*tanh(0.5*x_val[,4])+3*x_val[,5]*(2*I(x_val[,4]<1)-1)
fit <- rcate.rf(X,y,d,newdata=data.frame(x_val),method='RL')
y_pred <- fit$pred
plot(tau_val,y_pred);abline(0,1)
library(rqPen);library(splines)
Btilde.fun <- function(x, lambda2, knots2) {
B <- bs(x, degree = 3, knots = knots2, Boundary.knots = c(-4, 4))
D <- diff(diag(ncol(B)), differences = 2)
Omega <- crossprod(D)
n <- length(x)
M <- 1/n * crossprod(B) + lambda2 * Omega
R <- chol(M)
R1 <- solve(R)
Btilde.mat <- B %*% R1
return(Btilde.mat)
}
B_R <- function(x2, x3, lambda2br, knots.op1,colnum) {
Btilde0 <- NULL
result <- knot(x3, knots.op1)
for (i in 1:length(colnum)) {
Btilde1 <- Btilde.fun(x2[, i], lambda2br, result[, i])
Btilde0 <- cbind(Btilde0, Btilde1)
}
return(Btilde0)
}
# Genrate knots
knot <- function(x, knots.op) {
knot.mat = apply(x, 2, function(y)
quantile(y, probs = head(seq(0, 1, length.out = as.numeric(knots.op + 2)), -1)[-1]))
return(knot.mat)
}
# Adaptive INIS
adaptINIS <- function(x, y, testdata = NULL, lambda.pen.list = NULL,
folds = NULL, quant = NULL, kfold = NULL, knots = NULL, eps0 = 1e-06,
DOISIS = TRUE, maxloop = 10, trace = FALSE, detailed = FALSE) {
t0 = proc.time()[1]
cat("starting adaptINIS, NIS algorithm, adatively choose number of variables\n")
n <- nrow(x)
p <- ncol(x)
# if(is.null(nsis)) nsis=min(floor(n/log(n)),p-1)
if (is.null(knots)) {
knots = ceiling(n^0.2)
}
if (is.null(folds)) {
temp = sample(1:n, n, replace = FALSE)
if (is.null(kfold))
kfold = 5
for (i in 1:kfold) {
folds[[i]] = setdiff(1:n, temp[seq(i, n, kfold)])
}
}
if (is.null(quant)) {
quant = 1
}
df0 <- knots + 1
xbs = matrix(0, n, df0 * p)
for (i in 1:p) {
xbs[, (i - 1) * (df0) + (1:df0)] = ns(x[, i], df = df0)
}
tempresi <- rep(0, p)
curloop = 1
for (i in 1:p) {
tempfit <- lm.fit(x = cbind(1, xbs[, (i - 1) * df0 + 1:df0]), y = y)
tempresi[i] <- sum(tempfit$residuals^2)
}
used.list <- tempresi
used.sort <- sort(used.list, method = "sh", index = TRUE, decreasing = FALSE)
initRANKorder <- used.sort$ix
mindex <- sample(1:n)
mresi = NULL
for (i in 1:p) {
tempfit <- lm.fit(x = cbind(1, xbs[, (i - 1) * df0 + 1:df0]), y = y[mindex])
mresi[i] <- sum(tempfit$residuals^2)
}
resi.thres = quantile(mresi, 1 - quant)
nsis <- max(min(sum(used.list < resi.thres), floor(n/df0/3)), 2)
SISind <- sort(initRANKorder[1:nsis])
if (!DOISIS)
return(list(initRANKorder = initRANKorder, SISind = SISind, nsis = nsis))
cat("loop ", curloop, "...SISind ", SISind, "\n")
pick.ind = initRANKorder[1:nsis]
return(pick.ind)
}
#' p = 1/(1+exp(-X[,1]+X[,2]))
#' d = rbinom(n,1,p)
#' t = 2*d-1
#' y = 100+4*X[,1]+X[,2]-3*X[,3]+tau*t/2 + rnorm(n,0,1)
#' x_val = matrix(rnorm(200*10,0,1),nrow=200,ncol=10)
#' tau_val = 6*sin(2*x_val[,1])+3*(x_val[,2]+3)*x_val[,3]+9*tanh(0.5*x_val[,4])+3*x_val[,5]*(2*I(x_val[,4]<1)-1)
#'
#' fit <- rcate.am(X,y,d)
#' y_pred <- predict.rcate.am(fit,x_val)$pred
#' @export
rcate.am <- function(x, y, d, method = "MCMEA", NIS = TRUE, nknots = NA,
lambda.smooth = 2, nlambda = 30, nfolds = 5, n.trees.p = 40000,
shrinkage.p = 0.005, n.minobsinnode.p = 10,
interaction.depth.p = 1, cv.p = 2, n.trees.mu = c(1:50) * 50,
shrinkage.mu = 0.01,
n.minobsinnode.mu = 5, interaction.depth.mu = 5, cv.mu = 5) {
# Calculate T=2D-1
t <- 2 * d - 1
if (is.vector(x)) {
x <- matrix(x, ncol = 1)
}
# Number of rows and columns of X
row <- nrow(x)
col <- ncol(x)
# Standardize X
mean.x <- apply(x, 2, mean)
sd.x <- apply(x, 2, sd)
center.x <- (x - mean.x)/sd.x
# Calculate number of knots
if (is.na(nknots)) {
nknots <- floor(sqrt(row)/2)
}
# NIS
if (col < floor(row/log(row)) | NIS == FALSE) {
colnum <- 1:col
} else {
colnum_d <- adaptINIS(x, d)
colnum_y1 <- adaptINIS(x[d == 1, ], y[d == 1])
colnum_y0 <- adaptINIS(x[d == 0, ], y[d == 0])
colnum <- sort(unique(union(union(colnum_d, colnum_y0), colnum_y1)))
}
group <- rep(1:length(colnum), each = nknots + 3)
x.tr <- center.x[, colnum]
# If X has only one dimension, transform it into a matrix with one column
if (is.vector(x.tr)) {
x.tr <- matrix(x.tr, ncol = 1)
}
# Estimate mu0(x), mu1(x) and p(x)
data.p <- data.frame(cbind(factor(d), x))
colnames(data.p) <- c("d", paste0("X", 1:ncol(x)))
data.p$d <- as.factor(data.p$d)
gbmGrid.p <- expand.grid(interaction.depth = interaction.depth.p,
n.trees = n.trees.p, shrinkage = shrinkage.p,
n.minobsinnode = n.minobsinnode.p)
gbmFit.p <- caret::train(d ~ ., data = data.p, method = "gbm",
verbose = FALSE, trControl = trainControl(method = "cv", number = cv.p),
tuneGrid = gbmGrid.p)
pscore.hat <- predict(gbmFit.p, newdata = data.p, type = "prob")[, 2]
data00 <- data.frame(cbind(y, x, d))
colnames(data00) <- c("y", paste0("X", 1:ncol(x)), "d")
data020 <- data00[data00$d == 0, ]
data021 <- data00[data00$d == 1, ]
gbmGrid.mu <- expand.grid(interaction.depth = interaction.depth.mu,
n.trees = n.trees.mu, shrinkage = shrinkage.mu,
n.minobsinnode = n.minobsinnode.mu)
gbmFit.mu <- caret::train(y ~ ., data = data00[, -ncol(data00)],
method = "gbm", verbose = FALSE, trControl = trainControl(method = "cv",
number = cv.mu), tuneGrid = gbmGrid.mu, metric = "MAE")
gbmFit.mu1 <- caret::train(y ~ ., data = data021[, -ncol(data021)],
method = "gbm", verbose = FALSE, trControl = trainControl(method = "cv",
number = cv.mu), tuneGrid = gbmGrid.mu, metric = "MAE")
gbmFit.mu0 <- caret::train(y ~ ., data = data020[, -ncol(data020)],
method = "gbm", verbose = FALSE, trControl = trainControl(method = "cv",
number = cv.mu), tuneGrid = gbmGrid.mu, metric = "MAE")
mu0 <- predict(gbmFit.mu0, newdata = data00)
mu1 <- predict(gbmFit.mu1, newdata = data00)
mu.ea <- predict(gbmFit.mu, newdata = data00)
# Do transformation
if (method == "MCMEA") {
w.tr <- 1/(t * pscore.hat + (1 - t)/2)
y.tr <- (y - mu.ea) * w.tr
wmat.tr <- matrix(0, row, row)
diag(wmat.tr) <- w.tr * t/2
Btilde <- B_R(x.tr, x.tr, lambda.smooth, nknots, colnum)
x.tr1 <- wmat.tr %*% cbind(rep(1, row), Btilde)
} else if (method == "RL") {
w.tr <- 1
y.tr <- (y_tr - mu.ea) * w.tr
wmat.tr <- matrix(0, row, row)
diag(wmat.tr) <- w.tr * (t - 2 * pscore.hat + 1)/2
Btilde <- B_R(x.tr, x.tr, lambda2, nknots, colnum)
x.tr1 <- wmat.tr %*% cbind(rep(1, row), Btilde)
} else if (method == "DR") {
y.tr <- (t - 2 * pscore.hat + 1) * (y)/(2 * pscore.hat * (1 - pscore.hat)) +
(pscore.hat - d)/pscore.hat * mu1 + (pscore.hat - d)/(1 - pscore.hat) * mu0
w.tr <- abs((t - 2 * pscore.hat + 1)/(2 * pscore.hat * (1 - pscore.hat)))
}
# Fit the weighted LAD model with group SCAD
model <- rqPen::cv.rq.group.pen(x = x.tr1, y = y.tr, groups = as.factor(c(max(group)+1, group)),
tau = 0.5, penalty = "SCAD", intercept = FALSE,
nfolds = nfolds, criteria = "BIC", nlambda = nlambda,
cvFunc = "AE")
# Get coefficients and fitted value
coef <- coef(model)
fitted.values <- cbind(rep(1, nrow(Btilde)), Btilde) %*% coef
result <- list(model = model, method = method, algorithm = "SAM",
lambda.smooth = lambda.smooth, fitted.values = fitted.values,
x = x, y = y, d = d, mean.x = mean.x, sd.x = sd.x,
coef = coef, colnum = colnum, nknots = nknots)
class(result) <- "rcate.am"
return(result)
}
n <- 1000; p <- 10
X <- matrix(rnorm(n*p,0,1),nrow=n,ncol=p)
tau = 6*sin(2*X[,1])+3*(X[,2]+3)*X[,3]+9*tanh(0.5*X[,4])+3*X[,5]*(2*I(X[,4]<1)-1)
p = 1/(1+exp(-X[,1]+X[,2]))
d = rbinom(n,1,p)
t = 2*d-1
y = 100+4*X[,1]+X[,2]-3*X[,3]+tau*t/2 + rnorm(n,0,1)
x_val = matrix(rnorm(200*10,0,1),nrow=200,ncol=10)
tau_val = 6*sin(2*x_val[,1])+3*(x_val[,2]+3)*x_val[,3]+9*tanh(0.5*x_val[,4])+3*x_val[,5]*(2*I(x_val[,4]<1)-1)
fit <- rcate.am(X,y,d)
predict.rcate.am <- function(object, x) {
algorithm <- object$algorithm
model <- object$model
colnum <- object$colnum
center.x <- apply(object$x, 2, function(x) (x - object$mean.x)/object$sd.x)
center.xval <- apply(x, 2, function(x) (x - object$mean.x)/object$sd.x)
if (algorithm == "SAM") {
center.xval <- center.xval[, colnum]
center.x <- center.x[, colnum]
if (is.vector(center.xval) & is.vector(center.x)) {
center.xval <- matrix(center.xval, ncol = 1)
center.x <- matrix(center.x, ncol = 1)
}
Btilde.val <- B_R(center.xval, center.x, object$lambda.smooth, object$nknots, colnum)
predict <- cbind(rep(1, nrow(Btilde.val)), Btilde.val) %*% object$coef
}
return(list(predict = predict, x = x, algorithm = object$algorithm,
model = model, method = object$method))
}
y_pred <- predict.rcate.am(fit,x_val)$pred
plot(tau_val,y_pred);abline(0,1)
n <- 1000; p <- 10
X <- matrix(rnorm(n*p,0,1),nrow=n,ncol=p)
tau = 6*sin(2*x[,1])+3*(x[,2])+x[,3]+9*tanh(0.5*x[,4])+3*x[,5]
p = 1/(1+exp(-X[,1]+X[,2]))
d = rbinom(n,1,p)
t = 2*d-1
y = 100+4*X[,1]+X[,2]-3*X[,3]+tau*t/2 + rnorm(n,0,1)
x_val = matrix(rnorm(200*10,0,1),nrow=200,ncol=10)
tau_val = 6*sin(2*x_val[,1])+3*(x_val[,2])+x_val[,3]+9*tanh(0.5*x_val[,4])+3*x_val[,5]
fit <- rcate.am(X,y,d)
y_pred <- predict.rcate.am(fit,x_val)$pred
plot(tau_val,y_pred);abline(0,1)
n <- 1000; p <- 10
X <- matrix(rnorm(n*p,0,1),nrow=n,ncol=p)
tau = 6*sin(2*X[,1])+3*(X[,2])+X[,3]+9*tanh(0.5*X[,4])+3*X[,5]
p = 1/(1+exp(-X[,1]+X[,2]))
d = rbinom(n,1,p)
t = 2*d-1
y = 100+4*X[,1]+X[,2]-3*X[,3]+tau*t/2 + rnorm(n,0,1)
x_val = matrix(rnorm(200*10,0,1),nrow=200,ncol=10)
tau_val = 6*sin(2*x_val[,1])+3*(x_val[,2])+x_val[,3]+9*tanh(0.5*x_val[,4])+3*x_val[,5]
fit <- rcate.am(X,y,d)
y_pred <- predict.rcate.am(fit,x_val)$pred
plot(tau_val,y_pred);abline(0,1)
devtools::document()
devtools::document()
devtools::check()
devtools::check()
devtools::check()
library(e1071)
devtools::check()
devtools::check()
devtools::check()
devtools::check()
devtools::check()
function0 <- paste0("layer_dense(units=", n.cells.nn[1],
", activation='relu',input_shape=ncol(x)) %>% layer_dropout(",
dropout.nn[1], ") %>%")
function1 <- paste0("layer_dense(units=", n.cells.nn[2:length(n.cells.nn)],
", activation='relu') %>% layer_dropout(",
dropout.nn[2:length(dropout.nn)], ") %>%")
function2 <- paste0("keras_model_sequential() %>% ", function0,
do.call(paste, c(as.list(function1), sep = "")),
"layer_dense(units=1, activation='linear')")
eval(parse(text = function2))
function2
parse(text = function2)
model = eval(parse(text = function2))
# Generate the number of cells used in NN
if (is.na(n.cells.nn)) {
n.cells.nn <- c(ncol(x), ceiling(max(ncol(x) * 0.5,1)))
}
# Generate the dropout rate of NN
if (is.na(dropout.nn)) {
dropout.nn <- c(0.5, 0.5)
}
function0 <- paste0("layer_dense(units=", n.cells.nn[1],
", activation='relu',input_shape=ncol(x)) %>% layer_dropout(",
dropout.nn[1], ") %>%")
function1 <- paste0("layer_dense(units=", n.cells.nn[2:length(n.cells.nn)],
", activation='relu') %>% layer_dropout(",
dropout.nn[2:length(dropout.nn)], ") %>%")
function2 <- paste0("keras_model_sequential() %>% ", function0,
do.call(paste, c(as.list(function1), sep = "")),
"layer_dense(units=1, activation='linear')")
model = eval(parse(text = function2))
model
model %>% compile(loss = "mae", optimizer = "adam")
model %>% fit(x, y, epochs = epochs.nn, verbose = 0, sample_weight = w.tr)
w.tr
fitted.values <- model %>% predict(x)
devtools::check()
install.packages('Rtools')
devtools::check()
devtools::check()
writeLines('PATH="${RTOOLS40_HOME}\\usr\\bin;${PATH}"', con = "~/.Renviron")
Sys.which("make")
devtools::check()
install.packages("jsonlite", type = "source")
install.packages("jsonlite", type = "source")
devtools::check()
devtools::document()
devtools::check()
devtools::check()
devtools::check()
devtools::check()
devtools::check()
devtools::document()
devtools::check()
install.packages('qpdf')
devtools::check()
devtools::check()
devtools::check()
devtools::check()
devtools::check()
devtools::find_rtools()
install.packages('Rtools')
