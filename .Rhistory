library(vip)
p1 <- vip(
object = model,                     # fitted model
method = "permute",                 # permutation-based VI scores
num_features = ncol(train_x),       # default only plots top 10 features
pred_wrapper = fitted.values,            # user-defined prediction function
train = as.data.frame(train_x) ,    # training data
target = train_y,                   # response values used for training
metric = "rsquared",                # evaluation metric
# progress = "text"                 # request a text-based progress bar
)
p1 <- vip(
object = model,                     # fitted model
method = "permute",                 # permutation-based VI scores
num_features = ncol(train_x),       # default only plots top 10 features
pred_fun = fitted.values,            # user-defined prediction function
train = as.data.frame(train_x) ,    # training data
target = train_y,                   # response values used for training
metric = "rsquared",                # evaluation metric
# progress = "text"                 # request a text-based progress bar
)
p1 <- vip(
object = model,                     # fitted model
method = "permute",                 # permutation-based VI scores
num_features = ncol(train_x),       # default only plots top 10 features
pred_wrapper = fitted.values,            # user-defined prediction function
train = as.data.frame(train_x) ,    # training data
target = train_y,                   # response values used for training
metric = "rsquared",                # evaluation metric
# progress = "text"                 # request a text-based progress bar
)
p1 <- vip(
object = model,                     # fitted model
method = "permute",                 # permutation-based VI scores
num_features = ncol(train_x),       # default only plots top 10 features
pred_wrapper = function(object, newdata) {
rowMeans(predict(object, newdata)) %>%
as.vector()
},            # user-defined prediction function
train = as.data.frame(train_x) ,    # training data
target = train_y,                   # response values used for training
metric = "rsquared",                # evaluation metric
# progress = "text"                 # request a text-based progress bar
)
train_x = as.matrix(x)
train_y = as.matrix(y.tr)
# function0 <- paste0("layer_dense(units=", n.cells.nn[1],
#                     ", activation='relu',input_shape=ncol(x)) %>% layer_dropout(",
#                     dropout.nn[1], ") %>%")
# function1 <- paste0("layer_dense(units=", n.cells.nn[2:length(n.cells.nn)],
#                     ", activation='relu') %>% layer_dropout(",
#                     dropout.nn[2:length(dropout.nn)], ") %>%")
# function2 <- paste0("keras_model_sequential() %>% ", function0,
#                     do.call(paste, c(as.list(function1), sep = "")),
#                     "layer_dense(units=1, activation='linear')")
#
# model = eval(parse(text = function2))
#
# model %>% keras::compile(loss = "mae", optimizer = "adam")
#
# model %>% keras::fit(x, y, epochs = epochs.nn, verbose = 0, sample_weight = w.tr)
# fitted.values <- model %>% predict(x)
keras_model_simple_mlp <- function(use_bn = FALSE, use_dp = FALSE,
name = NULL) {
# define and return a custom model
keras::keras_model_custom(name = name, function(self) {
# create layers we'll need for the call (this code executes once)
self$dense1 <- keras::layer_dense(units = n.cells.nn[1], activation = "relu")
self$dense1.1 <- keras::layer_dense(units = n.cells.nn[1], activation = "relu")
self$dense1.5 <- keras::layer_dropout(rate = dropout.nn[1])
self$dense2 <- keras::layer_dense(units = n.cells.nn[2:length(n.cells.nn)], activation = "relu")
#self$dense2.5 <- keras::layer_dropout(rate = dropout.nn[2])
self$dense3 <- keras::layer_dense(units = 1, activation = "linear")
if (use_dp)
self$dp <- keras::layer_dropout(rate = 0.5)
if (use_bn)
self$bn <- keras::layer_batch_normalization(axis = -1)
# implement call (this code executes during training & inference)
function(inputs, mask = NULL) {
train_x <- self$dense1(inputs)
if (use_dp)
train_x <- self$dp(train_x)
if (use_bn)
train_x <- self$bn(train_x)
self$dense3(train_x)
}
})
}
model <- keras_model_simple_mlp()
keras::compile(model, loss = "mae", optimizer = "adam")
history <- keras::fit(model,train_x, train_y, epochs = epochs.nn, verbose = 0, sample_weight = w.tr)
fitted.values <- rowMeans(predict(model,train_x))
p1 <- vip(
object = model,                     # fitted model
method = "permute",                 # permutation-based VI scores
num_features = ncol(train_x),       # default only plots top 10 features
pred_wrapper = function(object, newdata) {
rowMeans(predict(object, newdata)) %>%
as.vector()
},            # user-defined prediction function
train = as.data.frame(train_x) ,    # training data
target = train_y,                   # response values used for training
metric = "rsquared",                # evaluation metric
# progress = "text"                 # request a text-based progress bar
)
p1 <- vip(
object = model,                     # fitted model
method = "permute",                 # permutation-based VI scores
num_features = ncol(x),       # default only plots top 10 features
pred_fun = pred_wrapper,            # user-defined prediction function
train = as.data.frame(x) ,    # training data
target = y,                   # response values used for training
metric = "rsquared",                # evaluation metric
# progress = "text"                 # request a text-based progress bar
)
pred_wrapper <- function(object, newdata) {
predict(object, x = as.matrix(newdata)) %>%
as.vector()
}
p1 <- vip(
object = model,                     # fitted model
method = "permute",                 # permutation-based VI scores
num_features = ncol(x),       # default only plots top 10 features
pred_fun = pred_wrapper,            # user-defined prediction function
train = as.data.frame(x) ,    # training data
target = y,                   # response values used for training
metric = "rsquared",                # evaluation metric
# progress = "text"                 # request a text-based progress bar
)
p1 <- vip(
object = model,                     # fitted model
method = "permute",                 # permutation-based VI scores
num_features = ncol(x),       # default only plots top 10 features
pred_wrapper = pred_wrapper,            # user-defined prediction function
train = as.data.frame(x) ,    # training data
target = y,                   # response values used for training
metric = "rsquared",                # evaluation metric
# progress = "text"                 # request a text-based progress bar
)
print(p1)
p1
p1
p1$data
data.gbm <- data.frame(cbind(y.tr, x))
colnames(data.gbm) <- c("y.tr", paste0("X", 1:ncol(x)))
model <- gbm::gbm(y.tr ~ ., data = data.gbm, distribution = "laplace",
weights = w.tr, n.trees = n.trees.gbm,
interaction.depth = interaction.depth.gbm)
df.x <- data.frame(x)
colnames(df.x) <- paste0("X", 1:ncol(x))
fitted.values <- gbm::predict.gbm(model, df.x, n.trees = n.trees.gbm)
history <- gbm.perf(model,method = 'OOB', oobag.curve = TRUE,plot.it = FALSE)
gbm::relative.influence(model)
importance <- gbm::relative.influence(model)
#'  }
#' @examples
#' n <- 1000; p <- 3
#' X <- matrix(runif(n*p,-3,3),nrow=n,ncol=p)
#' y = 1+sin(X[,1]) + rnorm(n,0,0.5)
#' df <- data.frame(y,X)
#' tree <- reg_tree_imp(y~X1+X2+X3,data=df,minsize=3,newdata=df,weights=rep(1,1000))
#' y_pred <- tree$pred
#' plot(y,y_pred)
#' @export
reg_tree_imp <- function(formula, data, minsize, newdata, weights) {
# coerce to data.frame
data <- as.data.frame(data)
row.names(data) <- seq(1:nrow(data))
newdata <- as.data.frame(newdata)
# handle formula
formula <- stats::terms.formula(formula)
# get the design matrix
X <- stats::model.matrix(formula, data)
# extract target
y <- data[, as.character(formula)[2]]
weight <- weights
# initialize while loop
do_splits <- TRUE
# create output data.frame with splitting rules and observations
tree_info <- data.frame(NODE = 1, NOBS = nrow(data), FILTER = NA, TERMINAL = "SPLIT",
IMP_GINI = NA, SPLIT = NA, stringsAsFactors = FALSE)
# keep splitting until there are only leafs left
while(do_splits) {
# which parents have to be splitted
to_calculate <- which(tree_info$TERMINAL == "SPLIT")
for (j in to_calculate) {
# handle root node
if (!is.na(tree_info[j, "FILTER"])) {
# subset data according to the filter
this_data <- subset(data, eval(parse(text = tree_info[j, "FILTER"])))
# get the design matrix
X <- stats::model.matrix(formula, this_data)
weight <- weights[as.numeric(row.names(X))]
} else {
this_data <- data
}
# estimate splitting criteria
splitting <- apply(X,  MARGIN = 2, FUN = wmed_var, y = this_data[, all.vars(formula)[1]], weight=weight)
# get the min SSE
tmp_splitter <- which.min(splitting[1,])
# define maxnode
mn <- max(tree_info$NODE)
# paste filter rules
current_filter <- c(paste(names(tmp_splitter), ">=",
splitting[2,tmp_splitter]),
paste(names(tmp_splitter), "<",
splitting[2,tmp_splitter]))
# Error handling! check if the splitting rule has already been invoked
split_here  <- !sapply(current_filter,
FUN = function(x,y) any(grepl(x, x = y)),
y = tree_info$FILTER)
# append the splitting rules
if (!is.na(tree_info[j, "FILTER"])) {
current_filter  <- paste(tree_info[j, "FILTER"],
current_filter, sep = " & ")
}
# calculate metrics within the children
metr <- lapply(current_filter,
FUN = function(i, x, data, formula) {
df <- subset(x = x, subset = eval(parse(text = i)))
nobs <- nrow(df)
w <- nobs/nrow(data)
y <- df[, all.vars(formula)[1]]
imp <- mean(abs(y - stats::median(y, na.rm = TRUE)))
return(c(nobs, w*imp))
},
x = this_data, data = data, formula = formula)
# extract relevant information
current_nobs <- sapply(metr, function(x) x[[1]])
imp_sum_child <- sum(sapply(metr, function(x) x[[2]]))
current_y <- this_data[, all.vars(formula)[1]]
imp_parent <- nrow(this_data)/nrow(data) * mean(abs(current_y-stats::median(current_y)))
imp_gini <- imp_parent - imp_sum_child
# insufficient minsize for split
if (any(current_nobs <= minsize)) {
split_here <- rep(FALSE, 2)
}
# create children data frame
children <- data.frame(NODE = c(mn+1, mn+2),
NOBS = current_nobs,
FILTER = current_filter,
TERMINAL = rep("SPLIT", 2),
IMP_GINI = NA,
SPLIT = NA,
row.names = NULL)[split_here,]
# overwrite state of current node, add gini importance and split variable
tree_info[j, "TERMINAL"] <- ifelse(all(!split_here), "LEAF", "PARENT")
tree_info[j, "IMP_GINI"] <- imp_gini
if (tree_info[j, "TERMINAL"] == "PARENT") {
tree_info[j, "SPLIT"] <- names(tmp_splitter)
}
# bind everything
tree_info <- rbind(tree_info, children)
# check if there are any open splits left
do_splits <- !all(tree_info$TERMINAL != "SPLIT")
} # end for
} # end while
# calculate fitted values
leafs <- tree_info[tree_info$TERMINAL == "LEAF", ]
fitted <- c()
nodepred <- rep(NA,nrow(leafs))
for (i in seq_len(nrow(leafs))) {
# extract index
ind <- as.numeric(rownames(subset(data, eval(parse(text = leafs[i, "FILTER"])))))
# estimator is the median y value of the leaf
fitted[ind] <- stats::median(y[ind])
nodepred[i] <- stats::median(y[ind])
}
# calculate predicted values
predicted <- c()
for (i in seq_len(nrow(leafs))) {
# extract index
ind <- as.numeric(rownames(subset(newdata, eval(parse(text = leafs[i, "FILTER"])))))
# estimator is the median y value of the leaf
predicted[ind] <- nodepred[i]
}
# calculate feature importance
imp <- tree_info[, c("SPLIT", "IMP_GINI")]
if (!all(is.na(imp$SPLIT))) {
imp <- stats::aggregate(IMP_GINI ~ SPLIT, FUN = function(x, all) sum(x, na.rm = T)/sum(all, na.rm = T),
data = imp, all = imp$IMP_GINI)
}
# rename to importance
names(imp) <- c("FEATURES", "IMPORTANCE")
imp <- imp[order(imp$IMPORTANCE, decreasing = TRUE),]
# return everything
return(list(tree = tree_info, fit = fitted, formula = formula,
importance = imp, data = data, pred=predicted, newdata=newdata))
}
n <- 1000; p <- 3
X <- matrix(runif(n*p,-3,3),nrow=n,ncol=p)
y = 1+sin(X[,1]) + rnorm(n,0,0.5)
df <- data.frame(y,X)
tree <- reg_tree_imp(y~X1+X2+X3,data=df,minsize=3,newdata=df,weights=rep(1,1000))
y_pred <- tree$pred
plot(y,y_pred)
plot(y,y_pred);abline(0,1)
tree$importance
formula=y~X1+X2+X3
n <- 1000; p <- 3
X <- matrix(runif(n*p,-3,3),nrow=n,ncol=p)
y = 1+sin(X[,1]) + rnorm(n,0,0.5)
df <- data.frame(y,X)
n_trees=50
feature_frac=1/2
data=newdata=df
weights=rep(1,1000)
n_trees=5
minnodes=5
# define function to sprout a single tree
sprout_tree <- function(formula, feature_frac, data, newdata, weights) {
# extract features
features <- all.vars(formula)[-1]
# extract target
target <- all.vars(formula)[1]
# bag the data
# - randomly sample the data with replacement (duplicate are possible)
train.id <- sample(1:nrow(data), size = nrow(data), replace = TRUE)
train <- data[train.id,]
w.train <- weights[train.id]
# randomly sample features
# - only fit the regression tree with feature_frac * 100 % of the features
features_sample <- sample(features,
size = ceiling(length(features) * feature_frac),
replace = FALSE)
# create new formula
formula_new <-
stats::as.formula(paste0(target, " ~ -1 + ", paste0(features_sample,
collapse =  " + ")))
# fit the regression tree
tree <- reg_tree_imp(formula = formula_new,
data = train, newdata=newdata,
minsize = minnodes, weights = w.train)
# save the fit and the importance
return(list(tree$fit, tree$importance, tree$pred))
}
# apply the rf_tree function n_trees times with plyr::raply
# - track the progress with a progress bar
trees <- plyr::raply(
n_trees,
sprout_tree(
formula = formula,
feature_frac = feature_frac,
data = data,
newdata = newdata,
weights = weights
),
.progress = "text"
)
# extract fit
fits <- do.call("cbind", trees[, 1])
preds <- do.call("cbind", trees[, 3])
# calculate the final fit as a mean of all regression trees
rf_fit <- apply(fits, MARGIN = 1, mean, na.rm = TRUE)
rf_pred <- apply(preds, MARGIN = 1, mean, na.rm = TRUE)
# extract the feature importance
imp_full <- do.call("rbind", trees[, 2])
View(imp_full)
# build the mean feature importance between all trees
imp <- aggregate(IMPORTANCE ~ FEATURES, FUN = mean, imp_full)
# build the ratio for interpretation purposes
imp$IMPORTANCE <- imp$IMPORTANCE / sum(imp$IMPORTANCE)
View(imp)
#'  }
#' @examples
#' n <- 1000; p <- 3
#' X <- matrix(runif(n*p,-3,3),nrow=n,ncol=p)
#' y = 1+sin(X[,1]) + rnorm(n,0,0.5)
#' df <- data.frame(y,X)
#' RF <- reg_rf(y~X1+X2+X3,data=df,newdata=df,weights=rep(1,1000),n_trees=5)
#' y_pred <- RF$pred
#' plot(y,y_pred)
#' @export
reg_rf <- function(formula, n_trees=50, feature_frac=1/2, data, newdata, weights, minnodes=5) {
# define function to sprout a single tree
sprout_tree <- function(formula, feature_frac, data, newdata, weights) {
# extract features
features <- all.vars(formula)[-1]
# extract target
target <- all.vars(formula)[1]
# bag the data
# - randomly sample the data with replacement (duplicate are possible)
train.id <- sample(1:nrow(data), size = nrow(data), replace = TRUE)
train <- data[train.id,]
w.train <- weights[train.id]
# randomly sample features
# - only fit the regression tree with feature_frac * 100 % of the features
features_sample <- sample(features,
size = ceiling(length(features) * feature_frac),
replace = FALSE)
# create new formula
formula_new <-
stats::as.formula(paste0(target, " ~ -1 + ", paste0(features_sample,
collapse =  " + ")))
# fit the regression tree
tree <- reg_tree_imp(formula = formula_new,
data = train, newdata=newdata,
minsize = minnodes, weights = w.train)
# save the fit and the importance
return(list(tree$fit, tree$importance, tree$pred))
}
# apply the rf_tree function n_trees times with plyr::raply
# - track the progress with a progress bar
trees <- plyr::raply(
n_trees,
sprout_tree(
formula = formula,
feature_frac = feature_frac,
data = data,
newdata = newdata,
weights = weights
),
.progress = "text"
)
# extract fit
fits <- do.call("cbind", trees[, 1])
preds <- do.call("cbind", trees[, 3])
# calculate the final fit as a mean of all regression trees
rf_fit <- apply(fits, MARGIN = 1, mean, na.rm = TRUE)
rf_pred <- apply(preds, MARGIN = 1, mean, na.rm = TRUE)
# extract the feature importance
imp_full <- do.call("rbind", trees[, 2])
# build the mean feature importance between all trees
imp <- aggregate(IMPORTANCE ~ FEATURES, FUN = mean, imp_full)
# build the ratio for interpretation purposes
imp$IMPORTANCE <- imp$IMPORTANCE / sum(imp$IMPORTANCE)
# export
return(list(fit = rf_fit, pred = rf_pred, importance = imp))
}
#' t = 2*d-1
#' y = 100+4*X[,1]+X[,2]-3*X[,3]+tau*t/2 + rnorm(n,0,1); set.seed(2223)
#' x_val = matrix(rnorm(200*3,0,1),nrow=200,ncol=3)
#' tau_val = 6*sin(2*x_val[,1])+3*(x_val[,2]+3)*x_val[,3]
#'
#' # Use MCM-EA transformation and GBM to estimate CATE
#' fit <- rcate.rf(X,y,d,newdata=data.frame(x_val),method='DR',feature.frac = 0.8, minnodes = 5)
#' y_pred <- fit$pred
#' plot(tau_val,y_pred);abline(0,1)
#' @export
rcate.rf <- function(x, y, d, method = "MCMEA",
n.trees.p = 40000, shrinkage.p = 0.005, n.minobsinnode.p = 10,
interaction.depth.p = 1, cv.p = 5, n.trees.mu = c(1:50) * 50,
shrinkage.mu = 0.01, n.minobsinnode.mu = 5,
interaction.depth.mu = 5, cv.mu = 5,
n.trees.rf = 50, feature.frac = 0.8, newdata = NULL, minnodes = 5) {
# Calculate T=2D-1
t <- 2 * d - 1
# Estimate mu0(x), mu1(x) and p(x)
data.p <- data.frame(cbind(d, x))
#colnames(data.p) <- c("d", paste0("X", 1:ncol(x)))
data.p$d <- as.factor(data.p$d)
gbmGrid.p <- expand.grid(interaction.depth = interaction.depth.p,
n.trees = n.trees.p, shrinkage = shrinkage.p,
n.minobsinnode = n.minobsinnode.p)
gbmFit.p <- caret::train(d ~ ., data = data.p, method = "gbm",
verbose = FALSE,
trControl = caret::trainControl(method = "cv", number = cv.p),
tuneGrid = gbmGrid.p)
pscore.hat <- caret::predict.train(gbmFit.p, newdata = data.p, type = "prob")[, 2]
data00 <- data.frame(cbind(y, x, d))
colnames(data00) <- c("y", paste0("X", 1:ncol(x)), "d")
data020 <- data00[data00$d == 0, ]
data021 <- data00[data00$d == 1, ]
gbmGrid.mu <- expand.grid(interaction.depth = interaction.depth.mu,
n.trees = n.trees.mu, shrinkage = shrinkage.mu,
n.minobsinnode = n.minobsinnode.mu)
gbmFit.mu <- caret::train(y ~ ., data = data00[, -ncol(data00)],
method = "gbm", verbose = FALSE,
trControl = caret::trainControl(method = "cv",
number = cv.mu), tuneGrid = gbmGrid.mu, metric = "MAE")
gbmFit.mu1 <- caret::train(y ~ ., data = data021[, -ncol(data021)],
method = "gbm", verbose = FALSE,
trControl = caret::trainControl(method = "cv",
number = cv.mu), tuneGrid = gbmGrid.mu, metric = "MAE")
gbmFit.mu0 <- caret::train(y ~ ., data = data020[, -ncol(data020)],
method = "gbm", verbose = FALSE,
trControl = caret::trainControl(method = "cv",
number = cv.mu), tuneGrid = gbmGrid.mu, metric = "MAE")
mu0 <- caret::predict.train(gbmFit.mu0, newdata = data00)
mu1 <- caret::predict.train(gbmFit.mu1, newdata = data00)
mu.ea <- caret::predict.train(gbmFit.mu, newdata = data00)
# Do transformation
if (method == "MCMEA") {
y.tr <- 2 * t * (y - mu.ea)
w.tr <- 1/2 * (t * pscore.hat + (1 - t)/2)
} else if (method == "RL") {
y.tr <- 2 * (y - mu.ea)/(t - 2 * pscore.hat + 1)
w.tr <- abs(t - 2 * pscore.hat + 1)/2
} else if (method == "DR") {
y.tr <- (t - 2 * pscore.hat + 1) * (y)/(2 * pscore.hat * (1 - pscore.hat)) +
(pscore.hat - d)/pscore.hat * mu1 + (pscore.hat - d)/(1 - pscore.hat) * mu0
w.tr <- abs((t - 2 * pscore.hat + 1)/(2 * pscore.hat * (1 - pscore.hat)))
}
data.rf <-  data.frame(y.tr,x)
if (is.null(newdata)) {newdata.rf <- data.rf} else {newdata.rf <- data.frame(newdata)}
formula.rf <- stats::as.formula(paste0('y.tr', " ~ ",paste0(colnames(data.frame(x)), collapse = " + ")))
result <- reg_rf(formula=formula.rf, n_trees = n.trees.rf, feature_frac = feature.frac,
data=data.rf, newdata=newdata.rf, weights = w.tr, minnodes = 5)
return(result)
}
n <- 1000; p <- 3
X <- matrix(rnorm(n*p,0,1),nrow=n,ncol=p); set.seed(2223)
tau = 6*sin(2*X[,1])+3*(X[,2]+3)*X[,3]
p = 1/(1+exp(-X[,1]+X[,2]))
d = rbinom(n,1,p)
t = 2*d-1
y = 100+4*X[,1]+X[,2]-3*X[,3]+tau*t/2 + rnorm(n,0,1); set.seed(2223)
x_val = matrix(rnorm(200*3,0,1),nrow=200,ncol=3)
tau_val = 6*sin(2*x_val[,1])+3*(x_val[,2]+3)*x_val[,3]
# Use MCM-EA transformation and GBM to estimate CATE
fit <- rcate.rf(X,y,d,newdata=data.frame(x_val),method='DR',feature.frac = 0.8, minnodes = 5)
fit$importance
